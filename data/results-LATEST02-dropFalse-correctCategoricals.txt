[1470 rows x 73 columns]
Classifier: LogisticRegression
Average accuracy: 0.8061224489795918

              precision    recall  f1-score   support

          No       0.94      0.81      0.87      1233
         Yes       0.43      0.74      0.54       237

    accuracy                           0.80      1470
   macro avg       0.68      0.77      0.71      1470
weighted avg       0.86      0.80      0.82      1470


Classifier: KNeighborsClassifier
Average accuracy: 0.5761904761904761

              precision    recall  f1-score   support

          No       0.92      0.55      0.69      1233
         Yes       0.24      0.75      0.37       237

    accuracy                           0.59      1470
   macro avg       0.58      0.65      0.53      1470
weighted avg       0.81      0.59      0.64      1470


Classifier: RandomForestClassifier
Average accuracy: 0.863265306122449

              precision    recall  f1-score   support

          No       0.88      0.98      0.93      1233
         Yes       0.71      0.31      0.43       237

    accuracy                           0.87      1470
   macro avg       0.79      0.64      0.68      1470
weighted avg       0.85      0.87      0.85      1470


Feature Importance:
['Age', 'DailyRate', 'DistanceFromHome', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'AverageSatisfaction', 'SalaryDeviation', 'BusinessTravel_Non-Travel', 'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'OverTime_No', 'OverTime_Yes', 'Education_1', 'Education_2', 'Education_3', 'Education_4', 'Education_5', 'EnvironmentSatisfaction_1', 'EnvironmentSatisfaction_2', 'EnvironmentSatisfaction_3', 'EnvironmentSatisfaction_4', 'JobInvolvement_1', 'JobInvolvement_2', 'JobInvolvement_3', 'JobInvolvement_4', 'JobLevel_1', 'JobLevel_2', 'JobLevel_3', 'JobLevel_4', 'JobLevel_5', 'JobSatisfaction_1', 'JobSatisfaction_2', 'JobSatisfaction_3', 'JobSatisfaction_4', 'RelationshipSatisfaction_1', 'RelationshipSatisfaction_2', 'RelationshipSatisfaction_3', 'RelationshipSatisfaction_4', 'StockOptionLevel_0', 'StockOptionLevel_1', 'StockOptionLevel_2', 'StockOptionLevel_3', 'WorkLifeBalance_1', 'WorkLifeBalance_2', 'WorkLifeBalance_3', 'WorkLifeBalance_4']
Sorted Feature Importance:
                          Feature  Importance
                      OverTime_No    0.072705
                     OverTime_Yes    0.071230
               StockOptionLevel_0    0.047243
             MaritalStatus_Single    0.037208
                    MonthlyIncome    0.032520
                       JobLevel_1    0.030268
                              Age    0.030193
                   YearsAtCompany    0.027079
                TotalWorkingYears    0.026933
              AverageSatisfaction    0.026561
             YearsWithCurrManager    0.026514
               StockOptionLevel_1    0.026357
                  SalaryDeviation    0.021600
                 DistanceFromHome    0.020411
                        DailyRate    0.019506
               YearsInCurrentRole    0.019392
        EnvironmentSatisfaction_1    0.017452
    JobRole_Laboratory Technician    0.016838
                       JobLevel_2    0.016703
               NumCompaniesWorked    0.016143
          YearsSinceLastPromotion    0.015865
            TrainingTimesLastYear    0.015713
                PercentSalaryHike    0.015587
            MaritalStatus_Married    0.015429
          JobRole_Sales Executive    0.014823
 BusinessTravel_Travel_Frequently    0.014644
                 JobInvolvement_3    0.013364
                JobSatisfaction_4    0.012894
           EducationField_Medical    0.012776
                JobSatisfaction_1    0.011623
                 JobInvolvement_1    0.011520
        EnvironmentSatisfaction_3    0.010375
                WorkLifeBalance_3    0.010363
                    Gender_Female    0.009561
                WorkLifeBalance_1    0.009372
       JobRole_Research Scientist    0.009181
                      Gender_Male    0.009000
     EducationField_Life Sciences    0.008765
       RelationshipSatisfaction_4    0.008476
        EnvironmentSatisfaction_4    0.008420
           MaritalStatus_Divorced    0.008331
     BusinessTravel_Travel_Rarely    0.008274
     JobRole_Sales Representative    0.008132
                      Education_3    0.007689
       RelationshipSatisfaction_1    0.007575
                JobSatisfaction_3    0.007189
                WorkLifeBalance_2    0.007188
        EnvironmentSatisfaction_2    0.006989
                 JobInvolvement_2    0.006961
       RelationshipSatisfaction_3    0.006956
                       JobLevel_3    0.006771
                      Education_4    0.006713
                      Education_2    0.005916
               StockOptionLevel_2    0.005785
  EducationField_Technical Degree    0.005588
         EducationField_Marketing    0.005005
                JobSatisfaction_2    0.004714
                 JobInvolvement_4    0.004685
                      Education_1    0.004608
   JobRole_Manufacturing Director    0.004428
       RelationshipSatisfaction_2    0.004376
JobRole_Healthcare Representative    0.003851
        BusinessTravel_Non-Travel    0.003470
                WorkLifeBalance_4    0.003211
               StockOptionLevel_3    0.002550
          JobRole_Human Resources    0.002298
             EducationField_Other    0.002131
                      Education_5    0.001897
                       JobLevel_4    0.001421
                  JobRole_Manager    0.001369
        JobRole_Research Director    0.001307
   EducationField_Human Resources    0.001206
                       JobLevel_5    0.000809


[73 rows x 2 columns]
Classifier: SVC
Average accuracy: 0.8578231292517007

              precision    recall  f1-score   support

          No       0.91      0.92      0.92      1233
         Yes       0.57      0.52      0.54       237

    accuracy                           0.86      1470
   macro avg       0.74      0.72      0.73      1470
weighted avg       0.85      0.86      0.86      1470

code used to generate:
########################################################################

# BUILD PIPELINE FOR ENCODING, SCALING AND FEATURE SELECTION

from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


# Redefine categorical and numerical feature names so they are based on
# whether they are actually categorical and numerical rather than on their
# initial data types. this is important for SMOTE and standard sampling
print(categoricalFeatureNames)
print(numericalFeatureNames)
print(len(categoricalFeatureNames), len(numericalFeatureNames))
# List of categorical features that are in "numericalFeatureNames" just because
# they were pre-encoded in the dataset
moveToCategorical = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 
                     'JobLevel', 'JobSatisfaction', 'RelationshipSatisfaction',
                      'StockOptionLevel',  'WorkLifeBalance']
for featureName in moveToCategorical:
    numericalFeatureNames.remove(featureName)
    categoricalFeatureNames.append(featureName)
print(len(categoricalFeatureNames), len(numericalFeatureNames))



# Define the numerical pipeline
numericalPipeline = Pipeline([
    ('scaler', StandardScaler())  # Numerical feature scaling
])

# First, we define the preprocessing steps in the preprocessor object. 
# It consists of a ColumnTransformer with two transformers: 
# numericalPipeline for scaling numerical features and OneHotEncoder() 
# for one-hot encoding categorical features.
# one hot encoder: drop=first: "dummy variable trap" avoidance.
preprocessor = ColumnTransformer([
    ('numerical', numericalPipeline, numericalFeatureNames),  # Apply scaling to numerical features
    #('categorical', OneHotEncoder(drop='first'), categoricalFeatureNames)  # One-hot encode categorical features 
    ('categorical', OneHotEncoder(), categoricalFeatureNames)  # One-hot encode categorical features 
])


# (use imb pipeline for SMOTE)
# The preprocessor is then included as part of the pipeline 
# along with other steps such as SMOTE oversampling and a 
# placeholder for the classification model.
pipeline = ImbPipeline([
    ('preprocessing', preprocessor),  # Preprocessing with separate transformations
    ('sampling', SMOTE()),  # Apply SMOTE for oversampling
    ('classification', None)  # Placeholder for the classification model
])




# Take a look at x_df after just the transformation step:
x_transformed = preprocessor.fit_transform(x_df)
print("Transformed Data:")
# Get the encoded feature names 
# Why tranformers [1][1]:
# first [1] to access second transformer in prepocessing (one hot encoding)
# second [1] because the information is stored as [transformer name, actual transformer]
encoded_feature_names = preprocessor.transformers_[1][1].get_feature_names_out()
# Combine numerical and categorical feature names
feature_names = numericalFeatureNames + encoded_feature_names.tolist()
#print(feature_names)
df_transformed =  pd.DataFrame(x_transformed, columns=feature_names)
print(df_transformed)

# having a quick look at the profiling report for the transformed data
if 0:
    from ydata_profiling import ProfileReport
    profile = ProfileReport(df_transformed, title="df_transformed")
    profile.to_file("data/reports/df_transformed.html")






# Define the list of classifiers to evaluate
classifiers = [
    LogisticRegression(max_iter=500),
    KNeighborsClassifier(n_neighbors=5),
    RandomForestClassifier(n_estimators=500),
    SVC(probability=True)
]

roc_data = []


# Next, we iterate over the classifiers in the classifiers list and 
# evaluate their performance using cross-validation. For each classifier,
# we set the current classifier in the pipeline using 
# pipeline.set_params(classification=classifier).


for classifier in classifiers:
    pipeline.set_params(classification=classifier)  # Set the current classifier
    scores = cross_val_score(pipeline, x_df, y_df, cv=5)  # Perform cross-validation
    print(f"Classifier: {classifier.__class__.__name__}")
    print(f"Average accuracy: {scores.mean()}")
    print("")
    # Perform cross-validation and get predicted labels (stratify by default)
    y_pred = cross_val_predict(pipeline, x_df, y_df, cv=5)      
    report = classification_report(y_df, y_pred)  # Generate classification report
    print(report)
    print("")

    # ROC curve
    # get the predicted probabilities for each data point
    y_scores = cross_val_predict(pipeline, x_df, y_df, cv=5, method='predict_proba')[:, 1] 
    # using pos_label = yes so that roc_curve understands 
    fpr, tpr, thresholds = roc_curve(y_df, y_scores,pos_label='Yes')
    auc = roc_auc_score(y_df, y_scores)
    roc_data.append((fpr, tpr, thresholds, classifier.__class__.__name__, auc))
    
    
    # Fit the pipeline to the entire dataset (x_df and y_df) to 
    # obtain feature importance and coefficient analysis if supported by the classifier
    pipeline.fit(x_df, y_df)
    

    if isinstance(classifier, RandomForestClassifier) and hasattr(classifier, 'feature_importances_'):
        print("Feature Importance:")

        #access the feature_importances_ attribute of the classifier  
        feature_importances = classifier.feature_importances_
        
        # Get the encoded feature names from the preprocessor step:
        encoded_feature_names = pipeline['preprocessing'].transformers_[1][1].get_feature_names_out()

        # Combine numerical and categorical feature names
        feature_names = numericalFeatureNames + encoded_feature_names.tolist()

        print(feature_names)
        #for feature_name, importance in zip(numericalFeatureNames + categoricalFeatureNames, feature_importances):
        #    print(f"{feature_name}: {importance}")
        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)
        print("Sorted Feature Importance:")
        print(importance_df)
        #print("")

    #if isinstance(classifier, LogisticRegression) and hasattr(pipeline['classification'], 'coef_'):
    #    print("Coefficient Analysis:")
    #    coefficients = pipeline['classification'].coef_
    #    feature_names = numericalFeatureNames + categoricalFeatureNames
    #    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})
    #    coefficients_df = coefficients_df.sort_values('Coefficient', ascending=False)
    #    print(coefficients_df)
    #    print("")
plt.figure()
# Plot the sorted DataFrame using a bar chart
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Sorted Feature Importance')
plt.xticks(rotation=20, ha='right')
plt.show()

# Calculate the cumulative importance
cumulative_importance = np.cumsum(importance_df['Importance'])

# Plot the cumulative importance using a line plot
plt.figure(figsize=(10, 6))
plt.plot(importance_df['Feature'], cumulative_importance)
plt.xlabel('Feature')
plt.ylabel('Cumulative Importance')
plt.title('Cumulative Feature Importance')
plt.xticks(rotation=20, ha='right')
plt.show()

quit()