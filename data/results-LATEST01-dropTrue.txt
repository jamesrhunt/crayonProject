[1470 rows x 41 columns]
Classifier: LogisticRegression
Average accuracy: 0.789795918367347

              precision    recall  f1-score   support

          No       0.93      0.79      0.86      1233
         Yes       0.40      0.71      0.51       237

    accuracy                           0.78      1470
   macro avg       0.67      0.75      0.68      1470
weighted avg       0.85      0.78      0.80      1470


Classifier: KNeighborsClassifier
Average accuracy: 0.6380952380952382

              precision    recall  f1-score   support

          No       0.92      0.64      0.75      1233
         Yes       0.27      0.69      0.39       237

    accuracy                           0.65      1470
   macro avg       0.59      0.67      0.57      1470
weighted avg       0.81      0.65      0.69      1470


Classifier: RandomForestClassifier
Average accuracy: 0.8666666666666666

              precision    recall  f1-score   support

          No       0.88      0.98      0.93      1233
         Yes       0.71      0.30      0.43       237

    accuracy                           0.87      1470
   macro avg       0.80      0.64      0.68      1470
weighted avg       0.85      0.87      0.84      1470


Feature Importance:
['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'AverageSatisfaction', 'SalaryDeviation', 'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Male', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 'JobRole_Sales Representative', 'MaritalStatus_Married', 'MaritalStatus_Single', 'OverTime_Yes']
Sorted Feature Importance:
                             Feature  Importance
40                      OverTime_Yes    0.134039
12                  StockOptionLevel    0.059479
6                           JobLevel    0.046520
39              MaritalStatus_Single    0.045185
8                      MonthlyIncome    0.035482
13                 TotalWorkingYears    0.033531
19              YearsWithCurrManager    0.032203
16                    YearsAtCompany    0.031886
20               AverageSatisfaction    0.031342
0                                Age    0.028640
21                   SalaryDeviation    0.027069
7                    JobSatisfaction    0.026793
2                   DistanceFromHome    0.026318
4            EnvironmentSatisfaction    0.025888
17                YearsInCurrentRole    0.025704
1                          DailyRate    0.024867
38             MaritalStatus_Married    0.024091
5                     JobInvolvement    0.023227
14             TrainingTimesLastYear    0.022913
9                 NumCompaniesWorked    0.022319
29                       Gender_Male    0.021792
18           YearsSinceLastPromotion    0.021481
36           JobRole_Sales Executive    0.020622
10                 PercentSalaryHike    0.020010
15                   WorkLifeBalance    0.019839
31     JobRole_Laboratory Technician    0.019077
22  BusinessTravel_Travel_Frequently    0.018925
3                          Education    0.017952
24      EducationField_Life Sciences    0.017928
11          RelationshipSatisfaction    0.016826
26            EducationField_Medical    0.014592
35        JobRole_Research Scientist    0.011716
37      JobRole_Sales Representative    0.011597
23      BusinessTravel_Travel_Rarely    0.009459
25          EducationField_Marketing    0.007576
33    JobRole_Manufacturing Director    0.007253
28   EducationField_Technical Degree    0.007218
30           JobRole_Human Resources    0.003420
27              EducationField_Other    0.002625
32                   JobRole_Manager    0.001737
34         JobRole_Research Director    0.000862
Classifier: SVC
Average accuracy: 0.8312925170068027

              precision    recall  f1-score   support

          No       0.90      0.90      0.90      1233
         Yes       0.49      0.50      0.50       237

    accuracy                           0.84      1470
   macro avg       0.70      0.70      0.70      1470
weighted avg       0.84      0.84      0.84      1470

code used to generate:
########################################################################

# BUILD PIPELINE FOR ENCODING, SCALING AND FEATURE SELECTION

from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming you have X and y as your feature and target variables
# Assuming you have separate lists num_features and cat_features containing the numerical and categorical feature names, respectively

# Define the pipeline
numericalPipeline = Pipeline([
    ('scaler', StandardScaler())  # Numerical feature scaling
])

# First, we define the preprocessing steps in the preprocessor object. 
# It consists of a ColumnTransformer with two transformers: 
# numericalPipeline for scaling numerical features and OneHotEncoder() 
# for one-hot encoding categorical features.
# one hot encoder: drop=first: "dummy variable trap" avoidance.
preprocessor = ColumnTransformer([
    ('numerical', numericalPipeline, numericalFeatureNames),  # Apply scaling to numerical features
    ('categorical', OneHotEncoder(drop='first'), categoricalFeatureNames)  # One-hot encode categorical features using pd.get_dummies()
    #('categorical', OneHotEncoder(), categoricalFeatureNames)  # One-hot encode categorical features using pd.get_dummies()
])


# (use imb pipeline for SMOTE)
# The preprocessor is then included as part of the pipeline 
# along with other steps such as SMOTE oversampling and a 
# placeholder for the classification model.
pipeline = ImbPipeline([
    ('preprocessing', preprocessor),  # Preprocessing with separate transformations
    ('sampling', SMOTE()),  # Apply SMOTE for oversampling
    ('classification', None)  # Placeholder for the classification model
])




# Take a look at x_df after just the transformation step:
x_transformed = preprocessor.fit_transform(x_df)
print("Transformed Data:")
# Get the encoded feature names 
# Why tranformers [1][1]:
# first [1] to access second transformer in prepocessing (one hot encoding)
# second [1] because the information is stored as [transformer name, actual transformer]
encoded_feature_names = preprocessor.transformers_[1][1].get_feature_names_out()
# Combine numerical and categorical feature names
feature_names = numericalFeatureNames + encoded_feature_names.tolist()
#print(feature_names)
df_transformed =  pd.DataFrame(x_transformed, columns=feature_names)
print(df_transformed)

# having a quick look at the profiling report for the transformed data
if 0:
    from ydata_profiling import ProfileReport
    profile = ProfileReport(df_transformed, title="df_transformed")
    profile.to_file("data/reports/df_transformed.html")






# Define the list of classifiers to evaluate
classifiers = [
    LogisticRegression(max_iter=500),
    KNeighborsClassifier(n_neighbors=5),
    RandomForestClassifier(n_estimators=500),
    SVC(probability=True)
]

roc_data = []


# Next, we iterate over the classifiers in the classifiers list and 
# evaluate their performance using cross-validation. For each classifier,
# we set the current classifier in the pipeline using 
# pipeline.set_params(classification=classifier).


for classifier in classifiers:
    pipeline.set_params(classification=classifier)  # Set the current classifier
    scores = cross_val_score(pipeline, x_df, y_df, cv=5)  # Perform cross-validation
    print(f"Classifier: {classifier.__class__.__name__}")
    print(f"Average accuracy: {scores.mean()}")
    print("")
    # Perform cross-validation and get predicted labels (stratify by default)
    y_pred = cross_val_predict(pipeline, x_df, y_df, cv=5)      
    report = classification_report(y_df, y_pred)  # Generate classification report
    print(report)
    print("")

    # ROC curve
    # get the predicted probabilities for each data point
    y_scores = cross_val_predict(pipeline, x_df, y_df, cv=5, method='predict_proba')[:, 1] 
    # using pos_label = yes so that roc_curve understands 
    fpr, tpr, thresholds = roc_curve(y_df, y_scores,pos_label='Yes')
    auc = roc_auc_score(y_df, y_scores)
    roc_data.append((fpr, tpr, thresholds, classifier.__class__.__name__, auc))
    
    
    # Fit the pipeline to the entire dataset (x_df and y_df) to 
    # obtain feature importance and coefficient analysis if supported by the classifier
    pipeline.fit(x_df, y_df)
    

    if isinstance(classifier, RandomForestClassifier) and hasattr(classifier, 'feature_importances_'):
        print("Feature Importance:")

        #access the feature_importances_ attribute of the classifier  
        feature_importances = classifier.feature_importances_
        
        # Get the encoded feature names from the preprocessor step:
        encoded_feature_names = pipeline['preprocessing'].transformers_[1][1].get_feature_names_out()

        # Combine numerical and categorical feature names
        feature_names = numericalFeatureNames + encoded_feature_names.tolist()

        print(feature_names)
        #for feature_name, importance in zip(numericalFeatureNames + categoricalFeatureNames, feature_importances):
        #    print(f"{feature_name}: {importance}")
        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)
        print("Sorted Feature Importance:")
        print(importance_df)
        #print("")

    #if isinstance(classifier, LogisticRegression) and hasattr(pipeline['classification'], 'coef_'):
    #    print("Coefficient Analysis:")
    #    coefficients = pipeline['classification'].coef_
    #    feature_names = numericalFeatureNames + categoricalFeatureNames
    #    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})
    #    coefficients_df = coefficients_df.sort_values('Coefficient', ascending=False)
    #    print(coefficients_df)
    #    print("")
plt.figure()
# Plot the sorted DataFrame using a bar chart
plt.figure(figsize=(10, 6))
plt.bar(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Sorted Feature Importance')
plt.xticks(rotation=20, ha='right')
plt.show()

# Calculate the cumulative importance
cumulative_importance = np.cumsum(importance_df['Importance'])

# Plot the cumulative importance using a line plot
plt.figure(figsize=(10, 6))
plt.plot(importance_df['Feature'], cumulative_importance)
plt.xlabel('Feature')
plt.ylabel('Cumulative Importance')
plt.title('Cumulative Feature Importance')
plt.xticks(rotation=20, ha='right')
plt.show()

quit()


fig, ax = plt.subplots()
ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference
for fpr, tpr, _, model_name, auc in roc_data:
    ax.plot(fpr, tpr, label=model_name+str(auc))
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('Receiver Operating Characteristic (ROC) Curve')
ax.legend()
plt.show()

quit()