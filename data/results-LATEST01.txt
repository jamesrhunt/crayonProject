[1470 rows x 47 columns]
Classifier: LogisticRegression
Average accuracy: 0.7802721088435375

              precision    recall  f1-score   support

          No       0.93      0.79      0.86      1233
         Yes       0.39      0.71      0.51       237

    accuracy                           0.78      1470
   macro avg       0.66      0.75      0.68      1470
weighted avg       0.85      0.78      0.80      1470


Classifier: KNeighborsClassifier
Average accuracy: 0.6421768707482994

              precision    recall  f1-score   support

          No       0.92      0.63      0.74      1233
         Yes       0.27      0.71      0.39       237

    accuracy                           0.64      1470
   macro avg       0.59      0.67      0.57      1470
weighted avg       0.81      0.64      0.69      1470


Classifier: RandomForestClassifier
Average accuracy: 0.8680272108843538

              precision    recall  f1-score   support

          No       0.89      0.97      0.93      1233
         Yes       0.71      0.35      0.46       237

    accuracy                           0.87      1470
   macro avg       0.80      0.66      0.70      1470
weighted avg       0.86      0.87      0.85      1470


Feature Importance:
['Age', 'DailyRate', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'MonthlyIncome', 'NumCompaniesWorked', 'PercentSalaryHike', 'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager', 'AverageSatisfaction', 'SalaryDeviation', 'BusinessTravel_Non-Travel', 'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely', 'EducationField_Human Resources', 'EducationField_Life Sciences', 'EducationField_Marketing', 'EducationField_Medical', 'EducationField_Other', 'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male', 'JobRole_Healthcare Representative', 'JobRole_Human Resources', 'JobRole_Laboratory Technician', 'JobRole_Manager', 'JobRole_Manufacturing Director', 'JobRole_Research Director', 'JobRole_Research Scientist', 'JobRole_Sales Executive', 
'JobRole_Sales Representative', 'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single', 'OverTime_No', 'OverTime_Yes']   
Sorted Feature Importance:
                              Feature  Importance
45                        OverTime_No    0.084044
46                       OverTime_Yes    0.083913
12                   StockOptionLevel    0.050121
0                                 Age    0.036753
16                     YearsAtCompany    0.035663
44               MaritalStatus_Single    0.035306
13                  TotalWorkingYears    0.034972
6                            JobLevel    0.034785
20                AverageSatisfaction    0.034221
8                       MonthlyIncome    0.032599
19               YearsWithCurrManager    0.029016
2                    DistanceFromHome    0.025959
21                    SalaryDeviation    0.025723
17                 YearsInCurrentRole    0.025553
7                     JobSatisfaction    0.023290
1                           DailyRate    0.023062
5                      JobInvolvement    0.022892
9                  NumCompaniesWorked    0.022835
4             EnvironmentSatisfaction    0.021827
15                    WorkLifeBalance    0.019580
18            YearsSinceLastPromotion    0.019447
23   BusinessTravel_Travel_Frequently    0.019067
40            JobRole_Sales Executive    0.019039
14              TrainingTimesLastYear    0.018301
10                  PercentSalaryHike    0.018145
26       EducationField_Life Sciences    0.017007
28             EducationField_Medical    0.016873
11           RelationshipSatisfaction    0.016747
43              MaritalStatus_Married    0.016233
3                           Education    0.015858
35      JobRole_Laboratory Technician    0.013587
41       JobRole_Sales Representative    0.012593
31                      Gender_Female    0.012299
32                        Gender_Male    0.011825
42             MaritalStatus_Divorced    0.010406
39         JobRole_Research Scientist    0.009358
24       BusinessTravel_Travel_Rarely    0.009207
30    EducationField_Technical Degree    0.009143
27           EducationField_Marketing    0.006971
37     JobRole_Manufacturing Director    0.006300
33  JobRole_Healthcare Representative    0.004543
22          BusinessTravel_Non-Travel    0.004017
34            JobRole_Human Resources    0.003599
29               EducationField_Other    0.002617
25     EducationField_Human Resources    0.001847
36                    JobRole_Manager    0.001830
38          JobRole_Research Director    0.001026
Classifier: SVC
Average accuracy: 0.8448979591836736

              precision    recall  f1-score   support

          No       0.91      0.91      0.91      1233
         Yes       0.52      0.51      0.51       237

    accuracy                           0.84      1470
   macro avg       0.71      0.71      0.71      1470
weighted avg       0.84      0.84      0.84      1470



code used to generate:
########################################################################

# BUILD PIPELINE FOR ENCODING, SCALING AND FEATURE SELECTION

from sklearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd
from sklearn.model_selection import cross_val_score, cross_val_predict
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Assuming you have X and y as your feature and target variables
# Assuming you have separate lists num_features and cat_features containing the numerical and categorical feature names, respectively

# Define the pipeline
numericalPipeline = Pipeline([
    ('scaler', StandardScaler())  # Numerical feature scaling
])

# First, we define the preprocessing steps in the preprocessor object. 
# It consists of a ColumnTransformer with two transformers: 
# numericalPipeline for scaling numerical features and OneHotEncoder() 
# for one-hot encoding categorical features.
# one hot encoder: drop=first: "dummy variable trap" avoidance.
preprocessor = ColumnTransformer([
    ('numerical', numericalPipeline, numericalFeatureNames),  # Apply scaling to numerical features
    #('categorical', OneHotEncoder(drop='first'), categoricalFeatureNames)  # One-hot encode categorical features using pd.get_dummies()
    ('categorical', OneHotEncoder(), categoricalFeatureNames)  # One-hot encode categorical features using pd.get_dummies()
])


# (use imb pipeline for SMOTE)
# The preprocessor is then included as part of the pipeline 
# along with other steps such as SMOTE oversampling and a 
# placeholder for the classification model.
pipeline = ImbPipeline([
    ('preprocessing', preprocessor),  # Preprocessing with separate transformations
    ('sampling', SMOTE()),  # Apply SMOTE for oversampling
    ('classification', None)  # Placeholder for the classification model
])




# Take a look at x_df after just the transformation step:
x_transformed = preprocessor.fit_transform(x_df)
print("Transformed Data:")
# Get the encoded feature names 
# Why tranformers [1][1]:
# first [1] to access second transformer in prepocessing (one hot encoding)
# second [1] because the information is stored as [transformer name, actual transformer]
encoded_feature_names = preprocessor.transformers_[1][1].get_feature_names_out()
# Combine numerical and categorical feature names
feature_names = numericalFeatureNames + encoded_feature_names.tolist()
#print(feature_names)
df_transformed =  pd.DataFrame(x_transformed, columns=feature_names)
print(df_transformed)

# having a quick look at the profiling report for the transformed data
if 0:
    from ydata_profiling import ProfileReport
    profile = ProfileReport(df_transformed, title="df_transformed")
    profile.to_file("data/reports/df_transformed.html")






# Define the list of classifiers to evaluate
classifiers = [
    LogisticRegression(max_iter=500),
    KNeighborsClassifier(n_neighbors=5),
    RandomForestClassifier(n_estimators=500),
    SVC(probability=True)
]

roc_data = []


# Next, we iterate over the classifiers in the classifiers list and 
# evaluate their performance using cross-validation. For each classifier,
# we set the current classifier in the pipeline using 
# pipeline.set_params(classification=classifier).


for classifier in classifiers:
    pipeline.set_params(classification=classifier)  # Set the current classifier
    scores = cross_val_score(pipeline, x_df, y_df, cv=5)  # Perform cross-validation
    print(f"Classifier: {classifier.__class__.__name__}")
    print(f"Average accuracy: {scores.mean()}")
    print("")
    # Perform cross-validation and get predicted labels (stratify by default)
    y_pred = cross_val_predict(pipeline, x_df, y_df, cv=5)      
    report = classification_report(y_df, y_pred)  # Generate classification report
    print(report)
    print("")

    # ROC curve
    # get the predicted probabilities for each data point
    y_scores = cross_val_predict(pipeline, x_df, y_df, cv=5, method='predict_proba')[:, 1] 
    # using pos_label = yes so that roc_curve understands 
    fpr, tpr, thresholds = roc_curve(y_df, y_scores,pos_label='Yes')
    auc = roc_auc_score(y_df, y_scores)
    roc_data.append((fpr, tpr, thresholds, classifier.__class__.__name__, auc))
    
    
    # Fit the pipeline to the entire dataset (x_df and y_df) to 
    # obtain feature importance and coefficient analysis if supported by the classifier
    pipeline.fit(x_df, y_df)
    

    if isinstance(classifier, RandomForestClassifier) and hasattr(classifier, 'feature_importances_'):
        print("Feature Importance:")

        #access the feature_importances_ attribute of the classifier  
        feature_importances = classifier.feature_importances_
        
        # Get the encoded feature names from the preprocessor step:
        encoded_feature_names = pipeline['preprocessing'].transformers_[1][1].get_feature_names_out()

        # Combine numerical and categorical feature names
        feature_names = numericalFeatureNames + encoded_feature_names.tolist()

        print(feature_names)
        #for feature_name, importance in zip(numericalFeatureNames + categoricalFeatureNames, feature_importances):
        #    print(f"{feature_name}: {importance}")
        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)
        print("Sorted Feature Importance:")
        print(importance_df)
        #print("")

    #if isinstance(classifier, LogisticRegression) and hasattr(pipeline['classification'], 'coef_'):
    #    print("Coefficient Analysis:")
    #    coefficients = pipeline['classification'].coef_
    #    feature_names = numericalFeatureNames + categoricalFeatureNames
    #    coefficients_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients[0]})
    #    coefficients_df = coefficients_df.sort_values('Coefficient', ascending=False)
    #    print(coefficients_df)
    #    print("")

fig, ax = plt.subplots()
ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line for reference
for fpr, tpr, _, model_name, auc in roc_data:
    ax.plot(fpr, tpr, label=model_name+str(auc))
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_title('Receiver Operating Characteristic (ROC) Curve')
ax.legend()
plt.show()

quit()